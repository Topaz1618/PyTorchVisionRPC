{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed37d340-be6d-4396-b765-e9267a50dbd6",
   "metadata": {},
   "source": [
    "# 使用本地模型 ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d53557-7caa-4a36-aed7-1d923830c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置缓存目录\n",
    "cache_dir = '/mnt/Demo/YOLO/'\n",
    "torch.hub.set_dir(cache_dir)\n",
    "\n",
    "colors = np.random.randint(125, 255, (80, 3))\n",
    "\n",
    "# 定义模型权重文件路径\n",
    "model_weights_path = 'yolov5s.pt'\n",
    "\n",
    "# 加载模型\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_weights_path)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# Model\n",
    "model = torch.hub.load('./', 'yolov5s', source='local')  # or yolov5m, yolov5l, yolov5x, custom\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 加载图像（或视频帧）进行目标检测\n",
    "image_path = 'bus.jpg'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# 进行目标检测\n",
    "results = model(img)\n",
    "\n",
    "# 显示检测结果\n",
    "# results.show()\n",
    "color = colors[int(random.randint(1, 10))]\n",
    "print(\"color\", color)\n",
    "\n",
    "# 或者获取检测到的对象信息并进行后续处理\n",
    "detected_objects = results.pandas().xyxy[0]\n",
    "print(detected_objects)\n",
    "\n",
    "for index, obj in detected_objects.iterrows():\n",
    "    x1, y1, x2, y2, conf, label = int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), obj[4], int(obj[5])\n",
    "    print(f\"x1:{x1} x2:{x2} y1:{y1} y2:{y2} conf:{conf} Label: {label}\")\n",
    "#     画出边界框\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "\n",
    "#     # 标签文本\n",
    "    label_text = f\"{model.names[label]}: {conf:.2f}\"\n",
    "\n",
    "#     # 在边界框上方显示类别标签和置信度\n",
    "    cv2.putText(img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "    \n",
    "cv2.imwrite('output.jpg', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6b8e3-0f70-4a2f-9bca-f89fce75626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# from ipdb import set_trace; set_trace();\n",
    "# sys.path.append('ultralytics_yolov5_master')  \n",
    "# from models import Model \n",
    "\n",
    "\n",
    "\n",
    "# 设置缓存目录\n",
    "cache_dir = '/mnt/Demo/YOLO/'\n",
    "\n",
    "colors = np.random.randint(125, 255, (80, 3))\n",
    "\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_weights_path)\n",
    "\n",
    "# model = Model('yolov5s', weights=model_weights_path)\n",
    "\n",
    "# 加载图像（或视频帧）进行目标检测\n",
    "image_path = 'helmet.png'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# 进行目标检测\n",
    "results = model(img)\n",
    "\n",
    "# 显示检测结果\n",
    "# results.show()\n",
    "color = colors[int(random.randint(1, 10))]\n",
    "print(\"color\", color)\n",
    "\n",
    "# 或者获取检测到的对象信息并进行后续处理\n",
    "detected_objects = results.pandas().xyxy[0]\n",
    "print(detected_objects)\n",
    "\n",
    "for index, obj in detected_objects.iterrows():\n",
    "    x1, y1, x2, y2, conf, label = int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), obj[4], int(obj[5])\n",
    "    print(f\"x1:{x1} x2:{x2} y1:{y1} y2:{y2} conf:{conf} Label: {label}\")\n",
    "#     画出边界框\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "\n",
    "#     # 标签文本\n",
    "    label_text = f\"{model.names[label]}: {conf:.2f}\"\n",
    "\n",
    "#     # 在边界框上方显示类别标签和置信度\n",
    "    cv2.putText(img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "    \n",
    "cv2.imwrite('output.jpg', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3b770-3499-4ca7-b90c-c4fae195e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from models.yolo import Model \n",
    "\n",
    "\n",
    "# 设置缓存目录\n",
    "# cache_dir = '/mnt/Demo/YOLO/'\n",
    "# torch.hub.set_dir(cache_dir)\n",
    "\n",
    "colors = np.random.randint(125, 255, (80, 3))\n",
    "\n",
    "# 定义模型权重文件路径\n",
    "model_weights_path = 'best.pt'\n",
    "model_cfg_path = 'models/yolov5s.yaml' \n",
    "\n",
    "\n",
    "model = Model(cfg=model_cfg_path)\n",
    "model.load_state_dict(torch.load(model_weights_path), strict=False)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# state_dict = torch.load(model_weights_path, map_location=torch.device('cpu'))\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "# 加载图像（或视频帧）进行目标检测\n",
    "image_path = 'helmet.png'\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(image.shape)  # 输出 (830, 1048, 3)\n",
    "\n",
    "# 将图像从 OpenCV 格式转换为 RGB 格式\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "input_tensor = np.transpose(image, (2, 0, 1))  # 将通道数放在第二个维度 [3, 830, 1048]\n",
    "input_tensor = np.expand_dims(input_tensor, axis=0)  # 添加批处理维度 [1, 3, 830, 1048]\n",
    "input_tensor = torch.from_numpy(input_tensor).to(device)\n",
    "input_tensor = input_tensor.to(torch.float32)\n",
    "results = model(input_tensor)\n",
    "\n",
    "# print(results)\n",
    "# 显示检测结果\n",
    "# results.show()\n",
    "# color = colors[int(random.randint(1, 10))]\n",
    "# print(\"color\", color)\n",
    "\n",
    "# # 或者获取检测到的对象信息并进行后续处理\n",
    "# detected_objects = results.pandas().xyxy[0]\n",
    "# print(detected_objects)\n",
    "\n",
    "# for index, obj in detected_objects.iterrows():\n",
    "#     x1, y1, x2, y2, conf, label = int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), obj[4], int(obj[5])\n",
    "#     print(f\"x1:{x1} x2:{x2} y1:{y1} y2:{y2} conf:{conf} Label: {label}\")\n",
    "# #     画出边界框\n",
    "#     cv2.rectangle(img, (x1, y1), (x2, y2), (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "\n",
    "# #     # 标签文本\n",
    "#     label_text = f\"{model.names[label]}: {conf:.2f}\"\n",
    "\n",
    "# #     # 在边界框上方显示类别标签和置信度\n",
    "#     cv2.putText(img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "    \n",
    "# cv2.imwrite('output.jpg', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee641e8-2279-42d5-9bf3-75eef8b39876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "image_path = 'helmet.png'\n",
    "\n",
    "\n",
    "# 读取图像\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "\n",
    "h, w, _ = image.shape\n",
    "\n",
    "# 将图像从 OpenCV 格式转换为 RGB 格式\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 将 NumPy 数组转换为 PIL 图像对象\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "\n",
    "# 定义转换\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((h, w)),  # 将图像大小调整为模型期望的大小\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# 应用转换\n",
    "input_tensor = preprocess(image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # 添加批处理维度\n",
    "\n",
    "print(input_tensor.shape)\n",
    "\n",
    "# 现在可以将图像张量传递给模型进行预测\n",
    "# results = model(input_tensor)\n",
    "\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247cd47-bb4e-4e87-b5fd-1561f90d5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "image_path = 'helmet.png'\n",
    "\n",
    "# 读取图像\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "print(image.shape)  # 输出 (830, 1048, 3)\n",
    "\n",
    "# 将图像从 OpenCV 格式转换为 RGB 格式\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "input_tensor = np.expand_dims(image, axis=0)  # 添加批处理维度 [1, 830, 1048, 3]\n",
    "# input_tensor = np.transpose(input_tensor, (0, 3, 1, 2))  # 调整维度顺序 [1, 3, 830, 1048]\n",
    "\n",
    "print(\"Tensor shape:\", input_tensor.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebf9b4-115a-4a65-9f19-7c9a54ffca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "image_path = 'helmet.png'\n",
    "\n",
    "# 读取图像\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "\n",
    "input_tensor = np.transpose(image, (2, 0, 1))  # 将通道数放在第二个维度 [3, 830, 1048]\n",
    "input_tensor = np.expand_dims(input_tensor, axis=0)  # 添加批处理维度 [1, 3, 830, 1048]\n",
    "input_tensor = torch.from_numpy(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb9888-0e43-44ef-a482-047063e83767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# from ipdb import set_trace; set_trace();\n",
    "# sys.path.append('ultralytics_yolov5_master')  \n",
    "# from models import Model \n",
    "\n",
    "\n",
    "\n",
    "# 设置缓存目录\n",
    "cache_dir = '/mnt/Demo/YOLO/'\n",
    "torch.hub.set_dir(cache_dir)\n",
    "\n",
    "colors = np.random.randint(125, 255, (80, 3))\n",
    "\n",
    "# 定义模型权重文件路径\n",
    "model_weights_path = 'yolov5s.pt'\n",
    "\n",
    "# 加载模型\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_weights_path)\n",
    "\n",
    "# model = Model('yolov5s', weights=model_weights_path)\n",
    "\n",
    "# 加载图像（或视频帧）进行目标检测\n",
    "image_path = 'bus.jpg'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# 进行目标检测\n",
    "results = model(img)\n",
    "\n",
    "# 显示检测结果\n",
    "# results.show()\n",
    "color = colors[int(random.randint(1, 10))]\n",
    "print(\"color\", color)\n",
    "\n",
    "# 或者获取检测到的对象信息并进行后续处理\n",
    "detected_objects = results.pandas().xyxy[0]\n",
    "print(detected_objects)\n",
    "\n",
    "for index, obj in detected_objects.iterrows():\n",
    "    x1, y1, x2, y2, conf, label = int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), obj[4], int(obj[5])\n",
    "    print(f\"x1:{x1} x2:{x2} y1:{y1} y2:{y2} conf:{conf} Label: {label}\")\n",
    "#     画出边界框\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "\n",
    "#     # 标签文本\n",
    "    label_text = f\"{model.names[label]}: {conf:.2f}\"\n",
    "\n",
    "#     # 在边界框上方显示类别标签和置信度\n",
    "    cv2.putText(img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "    \n",
    "cv2.imwrite('output.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626e184e-b507-4ce9-a548-ee056419849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ch' is an invalid keyword argument for Unpickler()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_439/1665117845.py\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 加载模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_weights_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ch' is an invalid keyword argument for Unpickler()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from models.yolo import Model\n",
    "\n",
    "\n",
    "colors = np.random.randint(125, 255, (80, 3))\n",
    "\n",
    "\n",
    "# 定义模型权重文件路径\n",
    "model_weights_path = './yolov5s.pt'\n",
    "\n",
    "model = Model(cfg='models/yolov5s.yaml', ch=3, nc=80)\n",
    "\n",
    "# 加载模型\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_weights_path)\n",
    "weights = torch.load(model_weights_path, ch=3, nc=80)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "\n",
    "# 加载图像（或视频帧）进行目标检测\n",
    "image_path = 'bus.jpg'\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# 进行目标检测\n",
    "results = model(img)\n",
    "\n",
    "# 显示检测结果\n",
    "# results.show()\n",
    "color = colors[int(1)]\n",
    "\n",
    "# 或者获取检测到的对象信息并进行后续处理\n",
    "detected_objects = results.pandas().xyxy[0]\n",
    "print(detected_objects)\n",
    "\n",
    "for obj in detected_objects:\n",
    "    # 获取边界框坐标和类别信息\n",
    "    x1, y1, x2, y2, conf, label = int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3]), obj[4], int(obj[5])\n",
    "\n",
    "    # 画出边界框\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "\n",
    "    # 标签文本\n",
    "    label_text = f\"{model.names[label]}: {conf:.2f}\"\n",
    "\n",
    "    # 在边界框上方显示类别标签和置信度\n",
    "    cv2.putText(img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (int(color[0]), int(color[1]), int(color[2])), 2)\n",
    "    \n",
    "    cv2.imwrite('output.jpg', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e89b0c-f441-4116-acb5-125a3127ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import platform\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "IMG_FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'  # include image suffixes\n",
    "VID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv'  # include video suffixes\n",
    "\n",
    "def autopad(k, p=None):  # kernel, padding\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else (x // 2 for x in k)  # auto-pad\n",
    "    return p\n",
    "\n",
    "class Ensemble(nn.ModuleList):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, augment=False, profile=False, visualize=False):\n",
    "        y = []\n",
    "        for module in self:\n",
    "            y.append(module(x, augment, profile, visualize)[0])\n",
    "        y = torch.cat(y, 1)  # nms ensemble\n",
    "        return y, None  # inference, train output\n",
    "\n",
    "def attempt_load(weights, map_location=None, inplace=True, fuse=True):\n",
    "    model = Ensemble()\n",
    "    for w in weights if isinstance(weights, list) else [weights]:\n",
    "        ckpt = torch.load(Path(str(w).strip().replace(\"'\", '')), map_location=map_location)  # load\n",
    "        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n",
    "        model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\n",
    "    for m in model.modules():\n",
    "        m.inplace = inplace  # torch 1.7.0 compatibility\n",
    "\n",
    "    if len(model) == 1:\n",
    "        return model[-1]  # return model\n",
    "    else:\n",
    "        print(f'Ensemble created with {weights}\\n')\n",
    "        for k in ['names']:\n",
    "            setattr(model, k, getattr(model[-1], k))\n",
    "        model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride  # max stride\n",
    "        return model  # return ensemble\n",
    "\n",
    "class DetectMultiBackend(nn.Module):\n",
    "    def __init__(self, weights='yolov5s.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False):\n",
    "        super().__init__()\n",
    "        w = str(weights[0] if isinstance(weights, list) else weights)\n",
    "        pt, jit, onnx, engine = self.model_type(w)  # get backend\n",
    "        fp16 &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16\n",
    "        if pt:\n",
    "            model = attempt_load(weights if isinstance(weights, list) else w, map_location=device)\n",
    "            stride = max(int(model.stride.max()), 32)  # model stride\n",
    "            names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
    "            model.half() if fp16 else model.float()\n",
    "            self.model = model\n",
    "        self.__dict__.update(locals())\n",
    "    def forward(self, im, augment=False, visualize=False, val=False):\n",
    "        if self.pt:\n",
    "            y = self.model(im, augment=augment, visualize=visualize)[0]\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y = torch.tensor(y, device=self.device)\n",
    "        return (y, []) if val else y\n",
    "    @staticmethod\n",
    "    def model_type(p='path/to/model.pt'):\n",
    "        suffixes = ['.pt', '.torchscript', '.onnx', '.engine']\n",
    "        p = Path(p).name\n",
    "        pt, jit, onnx, engine = (s in p for s in suffixes)\n",
    "        return pt, jit, onnx, engine\n",
    "\n",
    "class LoadImages:\n",
    "    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`\n",
    "    def __init__(self,img0, img_size=640, stride=32, auto=True):\n",
    "        self.img0 = img0\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.mode = 'image'\n",
    "        self.auto = auto\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        img0 = self.img0\n",
    "        # Read image\n",
    "        # img0 = cv2.imread(path)  # BGR\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]\n",
    "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "        img = np.ascontiguousarray(img)\n",
    "        return img, img0\n",
    "    def new_video(self, path):\n",
    "        self.frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    def __len__(self):\n",
    "        return self.nf  # number of files\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def box_area(box):\n",
    "    # box = xyxy(4,n)\n",
    "    return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
    "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    # IoU = inter / (area1 + area2 - inter)\n",
    "    return inter / (box_area(box1.T)[:, None] + box_area(box2.T) - inter)\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def non_max_suppression(prediction,\n",
    "                        conf_thres=0.25,\n",
    "                        iou_thres=0.45,\n",
    "                        classes=None,\n",
    "                        agnostic=False,\n",
    "                        multi_label=False,\n",
    "                        labels=(),\n",
    "                        max_det=300):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping bounding boxes\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.1 + 0.03 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + 5), device=x.device)\n",
    "            v[:, :4] = lb[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "        # Apply finite constraint\n",
    "        # if not torch.isfinite(x).all():\n",
    "        #     x = x[torch.isfinite(x).all(1)]\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            break  # time limit exceeded\n",
    "    return output\n",
    "\n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "    clip_coords(coords, img0_shape)\n",
    "    return coords\n",
    "\n",
    "def clip_coords(boxes, shape):\n",
    "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    if isinstance(boxes, torch.Tensor):  # faster individually\n",
    "        boxes[:, 0].clamp_(0, shape[1])  # x1\n",
    "        boxes[:, 1].clamp_(0, shape[0])  # y1\n",
    "        boxes[:, 2].clamp_(0, shape[1])  # x2\n",
    "        boxes[:, 3].clamp_(0, shape[0])  # y2\n",
    "    else:  # np.array (faster grouped)\n",
    "        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "\n",
    "FONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "\n",
    "def is_ascii(s=''):\n",
    "    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n",
    "    s = str(s)  # convert list, tuple, None, etc. to str\n",
    "    return len(s.encode().decode('ascii', 'ignore')) == len(s)\n",
    "\n",
    "def is_chinese(s='人工智能'):\n",
    "    # Is string composed of any Chinese characters?\n",
    "    return True if re.search('[\\u4e00-\\u9fff]', str(s)) else False\n",
    "\n",
    "def check_font(font=FONT):\n",
    "    # Download font to CONFIG_DIR if necessary\n",
    "    font = Path(font)\n",
    "    if not font.exists() and not (CONFIG_DIR / font.name).exists():\n",
    "        url = \"https://ultralytics.com/assets/\" + font.name\n",
    "        torch.hub.download_url_to_file(url, str(font), progress=False)\n",
    "\n",
    "def is_writeable(dir, test=False):\n",
    "    # Return True if directory has write permissions, test opening a file with write permissions if test=True\n",
    "    if test:  # method 1\n",
    "        file = Path(dir) / 'tmp.txt'\n",
    "        try:\n",
    "            with open(file, 'w'):  # open file with write permissions\n",
    "                pass\n",
    "            file.unlink()  # remove file\n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "    else:  # method 2\n",
    "        return os.access(dir, os.R_OK)  # possible issues on Windows\n",
    "\n",
    "def user_config_dir(dir='Ultralytics', env_var='YOLOV5_CONFIG_DIR'):\n",
    "    # Return path of user configuration directory. Prefer environment variable if exists. Make dir if required.\n",
    "    env = os.getenv(env_var)\n",
    "    if env:\n",
    "        path = Path(env)  # use environment variable\n",
    "    else:\n",
    "        cfg = {'Windows': 'AppData/Roaming', 'Linux': '.config', 'Darwin': 'Library/Application Support'}  # 3 OS dirs\n",
    "        path = Path.home() / cfg.get(platform.system(), '')  # OS-specific config dir\n",
    "        path = (path if is_writeable(path) else Path('/tmp')) / dir  # GCP and AWS lambda fix, only /tmp is writeable\n",
    "    path.mkdir(exist_ok=True)  # make if required\n",
    "    return path\n",
    "\n",
    "CONFIG_DIR = user_config_dir()  # Ultralytics settings dir\n",
    "\n",
    "class URLError(OSError):\n",
    "    def __init__(self, reason, filename=None):\n",
    "        self.args = reason,\n",
    "        self.reason = reason\n",
    "        if filename is not None:\n",
    "            self.filename = filename\n",
    "    def __str__(self):\n",
    "        return '<urlopen error %s>' % self.reason\n",
    "\n",
    "def check_pil_font(font=FONT, size=10):\n",
    "    # Return a PIL TrueType Font, downloading to CONFIG_DIR if necessary\n",
    "    font = Path(font)\n",
    "    font = font if font.exists() else (CONFIG_DIR / font.name)\n",
    "    try:\n",
    "        return ImageFont.truetype(str(font) if font.exists() else font.name, size)\n",
    "    except Exception:  # download if missing\n",
    "        try:\n",
    "            check_font(font)\n",
    "            return ImageFont.truetype(str(font), size)\n",
    "        except URLError:  # not online\n",
    "            return ImageFont.load_default()\n",
    "\n",
    "class Annotator:\n",
    "    if RANK in (-1, 0):\n",
    "        check_pil_font()  # download TTF if necessary\n",
    "\n",
    "    # YOLOv5 Annotator for train/val mosaics and jpgs and detect/hub inference annotations\n",
    "    def __init__(self, im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc'):\n",
    "        assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\n",
    "        self.pil = pil or not is_ascii(example) or is_chinese(example)\n",
    "        if self.pil:  # use PIL\n",
    "            self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
    "            self.draw = ImageDraw.Draw(self.im)\n",
    "            self.font = check_pil_font(font='Arial.Unicode.ttf' if is_chinese(example) else font,\n",
    "                                       size=font_size or max(round(sum(self.im.size) / 2 * 0.035), 12))\n",
    "        else:  # use cv2\n",
    "            self.im = im\n",
    "        self.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n",
    "\n",
    "    def box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
    "        # Add one xyxy box to image with label\n",
    "        if self.pil or not is_ascii(label):\n",
    "            self.draw.rectangle(box, width=self.lw, outline=color)  # box\n",
    "            if label:\n",
    "                w, h = self.font.getsize(label)  # text width, height\n",
    "                outside = box[1] - h >= 0  # label fits outside box\n",
    "                self.draw.rectangle(\n",
    "                    (box[0], box[1] - h if outside else box[1], box[0] + w + 1,\n",
    "                     box[1] + 1 if outside else box[1] + h + 1),\n",
    "                    fill=color,\n",
    "                )\n",
    "                # self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL>8.0\n",
    "                self.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\n",
    "        else:  # cv2\n",
    "            p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "            cv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\n",
    "            if label:\n",
    "                tf = max(self.lw - 1, 1)  # font thickness\n",
    "                w, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\n",
    "                outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
    "                p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "                cv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "                cv2.putText(self.im,\n",
    "                            label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "                            0,\n",
    "                            self.lw / 3,\n",
    "                            txt_color,\n",
    "                            thickness=tf,\n",
    "                            lineType=cv2.LINE_AA)\n",
    "\n",
    "    def rectangle(self, xy, fill=None, outline=None, width=1):\n",
    "        # Add rectangle to image (PIL-only)\n",
    "        self.draw.rectangle(xy, fill, outline, width)\n",
    "\n",
    "    def text(self, xy, text, txt_color=(255, 255, 255)):\n",
    "        # Add text to image (PIL-only)\n",
    "        w, h = self.font.getsize(text)  # text width, height\n",
    "        self.draw.text((xy[0], xy[1] - h + 1), text, fill=txt_color, font=self.font)\n",
    "\n",
    "    def result(self):\n",
    "        # Return annotated image as array\n",
    "        return np.asarray(self.im)\n",
    "\n",
    "class Colors:\n",
    "    # Ultralytics color palette https://ultralytics.com/\n",
    "    def __init__(self):\n",
    "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
    "        hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
    "               '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
    "        self.palette = [self.hex2rgb('#' + c) for c in hex]\n",
    "        self.n = len(self.palette)\n",
    "\n",
    "    def __call__(self, i, bgr=False):\n",
    "        c = self.palette[int(i) % self.n]\n",
    "        return (c[2], c[1], c[0]) if bgr else c\n",
    "\n",
    "    @staticmethod\n",
    "    def hex2rgb(h):  # rgb order (PIL)\n",
    "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "colors = Colors()  # create instance for 'from utils.plots import colors'\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e2a677-e111-471f-bc93-f6f9596e40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 224 layers, 7266973 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Upsample' object has no attribute 'recompute_scale_factor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_439/2031572551.py\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# img0 = cv2.imdecode(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_439/2031572551.py\u001b[0m in \u001b[0;36mdetect_img\u001b[0;34m(img0)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#把图片数据转换成张量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#进行检测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_det\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#对检测结果进行处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_439/1223747588.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize, val)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/Demo/YOLO/ultralytics_yolov5_master/models/yolo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# augmented inference, None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# single-scale inference, train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/Demo/YOLO/ultralytics_yolov5_master/models/yolo.py\u001b[0m in \u001b[0;36m_forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners,\n\u001b[0;32m--> 154\u001b[0;31m                              recompute_scale_factor=self.recompute_scale_factor)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Upsample' object has no attribute 'recompute_scale_factor'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "device = torch.device('cpu')\n",
    "model = DetectMultiBackend(\"yolov5s.pt\", device=device, dnn=False)\n",
    "if model.pt:\n",
    "    model.model.float()\n",
    "print(\"模型加载完成\")\n",
    "def detect_img(img0): #预测\n",
    "    device = torch.device('cpu')\n",
    "    stride, names = model.stride, model.names  #读取模型的步长和模型的识别类别\n",
    "    dataset = LoadImages(img0=img0, img_size=[640, 640], stride=stride, auto=False)  #对读取的图片进行格式化处理\n",
    "    # print(dataset)\n",
    "    for im, im0s in dataset:\n",
    "        im = (torch.from_numpy(im).to(device).float()/255)[None]  #把图片数据转换成张量\n",
    "        \n",
    "        pred = model(im, augment=False, visualize=False)   #进行检测\n",
    "        \n",
    "        det = non_max_suppression(pred, 0.25, 0.45, None, False, max_det=1000)[0]  #对检测结果进行处理\n",
    "        \n",
    "        im0 = im0s.copy()\n",
    "        annotator = Annotator(im0, line_width=0, example=str(names))\n",
    "        \n",
    "        data = dict.fromkeys(names,0)\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()  #识别结果都在这里了\n",
    "\n",
    "            for *xyxy, conf, cls in reversed(det):   #xyxy是识别结果的标注框的坐标，conf是识别结果的置信度，cls是识别结果对应的类别\n",
    "                c = int(cls)\n",
    "                data[names[c]]+=1\n",
    "                label = (f'{names[c][0]}{data[names[c]]}')\n",
    "                annotator.box_label(xyxy, label, color=colors(c, True))  #对图片进行标注，就是画框\n",
    "\n",
    "        im0 = annotator.result()\n",
    "        return data,im0\n",
    "    \n",
    "img = cv2.imread('bus.jpg')\n",
    "\n",
    "img0 = cv2.imdecode(img)\n",
    "count,im0 = detect_img(img)\n",
    "print(count)\n",
    "\n",
    "image = cv2.imencode('.jpg',im0)[1]\n",
    "img = str(base64.b64encode(image))[2:-1]\n",
    "# cv2.imshow('a1',img1)\n",
    "with open('ai.jpg','wb') as f:\n",
    "        f.write(base64.b64decode(img))\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8eccd1-f011-4f5e-bc41-c04e6640f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2023-11-21 Python-3.8.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 224 layers, 7266973 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "image 1/1: 1080x810 4 persons, 1 bus, 1 fire hydrant\n",
      "Speed: 17.5ms pre-process, 51.3ms inference, 1.7ms NMS per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    " \n",
    "# GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# Model\n",
    "model = torch.hub.load('./', 'yolov5s', source='local')  # or yolov5m, yolov5l, yolov5x, custom\n",
    "model = model.to(device)\n",
    "# Images\n",
    "img = 'bus.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
    " \n",
    "# Inference\n",
    "results = model(img)\n",
    " \n",
    "# Results\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c82f4-c96f-4c07-ac64-db3cd549f779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
